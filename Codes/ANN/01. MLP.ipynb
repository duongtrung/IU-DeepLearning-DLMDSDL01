{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ef6840-e2c7-447a-bd7c-d49017b93dd9",
   "metadata": {},
   "source": [
    "#### Importing all dependencies\n",
    "\n",
    "The first step here is to add all the dependencies. We need os for file input/output functionality, as we will save the CIFAR-10 dataset to local disk later in this tutorial. We'll also import torch, which imports PyTorch. From it we import nn, which allows us to define a neural network module. We also import the DataLoader (for feeding data into the MLP during training), the CIFAR10 dataset (for obvious purposes) and transforms, which allows us to perform transformations on the data prior to feeding it to the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30675e9d-ba27-4fa3-9825-7874ba4961e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6b033-893d-4d4b-9338-3717052bb5c8",
   "metadata": {},
   "source": [
    "#### Defining the MLP neural network class\n",
    "\n",
    "Next up is defining the MLP class, which replicates the nn.Module class. This Module class instructs the implementation of our neural network and is therefore really useful when creating one. It has two definitions: __init__, or the constructor, and forward, which implements the forward pass.\n",
    "\n",
    "In the constructor, we first invoke the superclass initialization and then define the layers of our neural network. We stack all layers (three densely-connected layers with Linear and ReLU activation functions using nn.Sequential. We also add nn.Flatten() at the start. Flatten converts the 3D image representations (width, height and channels) into 1D format, which is necessary for Linear layers. Note that with image data it is often best to use Convolutional Neural Networks. This is out of scope for this tutorial and will be covered in another one.\n",
    "\n",
    "The forward pass allows us to react to input data - for example, during the training process. In our case, it does nothing but feeding the data through the neural network layers, and returning the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0275c0-3a4f-4d94-bc5e-a81908a15ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 32 * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1545852-01c9-42ee-93a7-98b93f7fbf5b",
   "metadata": {},
   "source": [
    "After defining the class, we can move on and write the runtime code. This code is actually executed at runtime, i.e. when you call the Python script from the terminal with e.g. python mlp.py. The class itself is then not yet used, but we will do so shortly.\n",
    "\n",
    "The first thing we define in the runtime code is setting the seed of the random number generator. Using a fixed seed ensures that this generator is initialized with the same starting value. This benefits reproducibility of your ML findings.\n",
    "\n",
    "Loading and preparing the CIFAR-10 data is a two-step process:\n",
    "\n",
    "- Initializing the dataset itself, by means of CIFAR10. Here, in increasing order, you specify the directory where the dataset has to be saved, that it must be downloaded, and that they must be converted into Tensor format.\n",
    "\n",
    "- Initializing the DataLoader, which takes the dataset, a batch size, shuffle parameter (whether the data must be ordered at random) and the number of workers to load data with. In PyTorch, data loaders are used for feeding data to the model uniformly.\n",
    "\n",
    "Next, we initialize the MLP. We also specify the loss function (categorical crossentropy loss) and the Adam optimizer. The optimizer works on the parameters of the MLP and utilizes a learning rate of 10e-4\n",
    "\n",
    "#### Defining the training loop\n",
    "\n",
    "The core part of our runtime code is the training loop. In this loop, we perform the epochs, or training iterations. For every iteration, we iterate over the training dataset, perform the entire forward and backward passes, and perform model optimization.\n",
    "\n",
    "Step-by-step, these are the things that happen within the loop:\n",
    "\n",
    "- Here, we use 5 epochs, as defined by the range(0, 5).\n",
    "\n",
    "- We set the current loss value for printing to 0.0.\n",
    "\n",
    "- Per epoch, we iterate over the training dataset - and more specifically, the minibatches within this training dataset as specified by the batch size (set in the trainloader above). Here, we do the following things:\n",
    "    - We decompose the data into inputs and targets (or x and y values, respectively).\n",
    "    \n",
    "    - We zero the gradients in the optimizer, to ensure that it starts freshly for this minibatch.\n",
    "    \n",
    "    - We perform the forward pass - which in effect is feeding the inputs to the model, which, recall, was initialized as mlp.\n",
    "    \n",
    "    - We then compute the loss value based on the outputs of the model and the ground truth, available in targets.\n",
    "    \n",
    "    - This is followed by the backward pass, where the gradients are computed, and optimization, where the model is adapted.\n",
    "    \n",
    "    - Finally, we print some statistics - but only at every 500th minibatch. At the end of the entire process, we print that the training process has finished.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b619e009-dfe9-4519-b9c8-36b96a70c53e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 2.23692\n",
      "Loss after mini-batch  1000: 2.09935\n",
      "Loss after mini-batch  1500: 2.03072\n",
      "Loss after mini-batch  2000: 1.99956\n",
      "Loss after mini-batch  2500: 1.93845\n",
      "Loss after mini-batch  3000: 1.94456\n",
      "Loss after mini-batch  3500: 1.91624\n",
      "Loss after mini-batch  4000: 1.90289\n",
      "Loss after mini-batch  4500: 1.86960\n",
      "Loss after mini-batch  5000: 1.85643\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 1.83056\n",
      "Loss after mini-batch  1000: 1.83181\n",
      "Loss after mini-batch  1500: 1.82517\n",
      "Loss after mini-batch  2000: 1.82263\n",
      "Loss after mini-batch  2500: 1.81629\n",
      "Loss after mini-batch  3000: 1.81058\n",
      "Loss after mini-batch  3500: 1.80163\n",
      "Loss after mini-batch  4000: 1.77174\n",
      "Loss after mini-batch  4500: 1.77388\n",
      "Loss after mini-batch  5000: 1.76247\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 1.75368\n",
      "Loss after mini-batch  1000: 1.76395\n",
      "Loss after mini-batch  1500: 1.74717\n",
      "Loss after mini-batch  2000: 1.75415\n",
      "Loss after mini-batch  2500: 1.72494\n",
      "Loss after mini-batch  3000: 1.72552\n",
      "Loss after mini-batch  3500: 1.73239\n",
      "Loss after mini-batch  4000: 1.70997\n",
      "Loss after mini-batch  4500: 1.71483\n",
      "Loss after mini-batch  5000: 1.68630\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 1.71741\n",
      "Loss after mini-batch  1000: 1.69296\n",
      "Loss after mini-batch  1500: 1.69440\n",
      "Loss after mini-batch  2000: 1.68808\n",
      "Loss after mini-batch  2500: 1.67944\n",
      "Loss after mini-batch  3000: 1.67779\n",
      "Loss after mini-batch  3500: 1.68294\n",
      "Loss after mini-batch  4000: 1.67324\n",
      "Loss after mini-batch  4500: 1.66446\n",
      "Loss after mini-batch  5000: 1.67616\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 1.67048\n",
      "Loss after mini-batch  1000: 1.65597\n",
      "Loss after mini-batch  1500: 1.65300\n",
      "Loss after mini-batch  2000: 1.63295\n",
      "Loss after mini-batch  2500: 1.64384\n",
      "Loss after mini-batch  3000: 1.66414\n",
      "Loss after mini-batch  3500: 1.64804\n",
      "Loss after mini-batch  4000: 1.62235\n",
      "Loss after mini-batch  4500: 1.64542\n",
      "Loss after mini-batch  5000: 1.64259\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Prepare CIFAR-10 dataset\n",
    "    dataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Initialize the MLP\n",
    "    mlp = MLP()\n",
    "  \n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Run the training loop\n",
    "    for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "      \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "      \n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "      \n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "      \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "      \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "      \n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print('Loss after mini-batch %5d: %.5f' % (i + 1, current_loss / 500))\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadf7d5-eeac-4dfc-8aa2-5d2835c44b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
